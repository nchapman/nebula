* [[https://github.com/nchapman/nebula/blob/main/examples/basic.rs][basic]]

** code
#+BEGIN_SRC Rust
use std::io::Write;

use nebula::{
    options::{ContextOptions, ModelOptions},
    Model,
};

fn main() {
    let args: Vec<String> = std::env::args().collect();
    if args.len() < 3 {
        println!(
            "usage:\n\t{} <model_file_path> \"<promt>\" <n_len = 150>",
            args[0]
        );
        return;
    }
    let model_file_name = args[1].clone();
    let prompt = args[2].clone();
    let mut n_len = None;
    if args.len() > 3 {
        if let Ok(ss) = str::parse(&args[3]) {
            n_len = Some(ss);
        }
    }
    let model_options = ModelOptions::default().with_n_gpu_layers(10);

    let model = Model::new(model_file_name, model_options).unwrap();

    let ctx_options = ContextOptions::default();

    let mut ctx = model.context(ctx_options).unwrap();

    ctx.eval_str(&prompt, true).unwrap();

    let mut p = ctx
        .predict()
        .with_token_callback(Box::new(|token| {
            print!("{}", token);
            std::io::stdout().flush().unwrap();
            true
        }))
        .with_temp(0.8);
    if let Some(n_len) = n_len {
        p = p.with_max_len(n_len);
    }
    p.predict().unwrap();

    println!("");
}
#+END_SRC
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example basic --features llama -- <model_file_path> "<promt>" [<n_len = 150>]
#+END_SRC
** models
*** llava-1.6-mistral-7b-gguf
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/llava-v1.6-mistral-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Mistral-7B-Instruct-v0.2-GGUF
**** Q5_K_M
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC

**** Q4_K_M
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Mixtral-8x7B-Instruct-v0.1-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** rocket-3B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/rocket-3B-GGUF/resolve/main/rocket-3b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/rocket-3b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** zephyr-7B-beta-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/zephyr-7b-beta.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** phi-2-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/phi-2.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Yi-34B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Yi-34B-GGUF/resolve/main/yi-34b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/yi-34b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Hermes-2-Pro-Mistral-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** gemma-7b-it
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf
  cd -
  cargo r --release --example basic --features llama -- models/gemma-7b-it.gguf "Write helloworld code in Rust."
#+END_SRC
*** StarCoder2-15B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/second-state/StarCoder2-15B-GGUF/resolve/main/starcoder2-15b-Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/starcoder2-15b-Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** dolphin-2.6-mistral-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/dolphin-2.6-mistral-7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Nous-Hermes-2-SOLAR-10.7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF/resolve/main/nous-hermes-2-solar-10.7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/nous-hermes-2-solar-10.7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Llama-2-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/raw/resolve/llama-2-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/llama-2-7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC


* [[https://github.com/nchapman/nebula/blob/main/examples/basic_with_image.rs][basic_with_image]]
** code
#+BEGIN_SRC
use std::io::{Read, Write};

use nebula::{
    options::{ContextOptions, ModelOptions},
    Model,
};

fn main() {
    let args: Vec<String> = std::env::args().collect();
    if args.len() < 4 {
        println!(
            "usage:\n\t{} <model_file_path> <mmproj_model_file_path> <image_file_name> \"<promt = Provide a full description.>\" <n_len = 6000>",
            args[0]
        );
        return;
    }
    let model_file_name = args[1].clone();
    let mmproj_model_file_name = args[2].clone();
    let image_file_name = args[3].clone();
    let prompt = if args.len() > 4 && !args[4].is_empty() {
        args[4].clone()
    } else {
        "Provide a full description.".to_string()
    };
    let mut n_len = 6000;
    if args.len() > 5 {
        if let Ok(ss) = str::parse(&args[5]) {
            n_len = ss;
        }
    }
    let model_options = ModelOptions::default().with_n_gpu_layers(10);
    let model =
        Model::new_with_mmproj(model_file_name, mmproj_model_file_name, model_options).unwrap();

    let context_options = ContextOptions::default().with_n_ctx(6000);
    let mut ctx = model.context(context_options).unwrap();

    //read image
    let mut image_bytes = vec![];
    let mut f = std::fs::File::open(&image_file_name).unwrap();
    f.read_to_end(&mut image_bytes).unwrap();

    //eval data
    ctx.eval_image(image_bytes, &prompt).unwrap();

    //generate predict

    //    let answer = ctx.predict(n_len).unwrap();
    //    println!("{answer}");

    //or
    ctx.predict()
        .with_token_callback(Box::new(|token| {
            print!("{}", token);
            std::io::stdout().flush().unwrap();
            true
        }))
        .with_max_len(n_len)
        .predict()
        .unwrap();
    println!("");
}
#+END_SRC
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example basic_with_image --features llama  -- <model_file_path> <mmproj_model_file_path> <image_file_name> ["<promt = <image>\nUSER:\nProvide a full description.\nASSISTANT:\n>"] [<n_len = 4000>]
#+END_SRC
** models
*** llava-1.6-mistral-7b-gguf
#+BEGIN_SRC bash
    mkdir models
    cd models
    wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/llava-v1.6-mistral-7b.Q4_K_M.gguf
    wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/mmproj-model-f16.gguf
    cd -
    cargo r --release --example basic_with_image --features llama -- models/llava-v1.6-mistral-7b.Q4_K_M.gguf models/mmproj-model-f16.gguf ~/red-fox-300x300.jpg
#+END_SRC


* [[https://github.com/nchapman/nebula/blob/main/examples/whisper_on_wav.rs][whisper_on_wav]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example whisper_on_wav --features whisper
#+END_SRC
** models
*** ggml-base.en.bin
#+BEGIN_SRC bash
    mkdir models
    wget -O models/ggml-base.en.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin?download=true
    cargo r --release --example whisper_on_wav --features whisper
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/main/examples/whisper_stream.rs][whisper_stream]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example whisper_stream --features whisper
#+END_SRC
** models
*** ggml-base.en.bin
#+BEGIN_SRC bash
    mkdir models
    wget -O models/ggml-base.en.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin?download=true
    cargo r --release --example whisper_stream --features whisper
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/PUL-122-embeddings-to-nebula/examples/embeddings.rs][embeddings]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example embeddings --features embeddings -- <model_type>
#+END_SRC
** models
*** jina
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- jina
#+END_SRC
*** t5
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- t5
#+END_SRC
*** bert
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- bert
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/main/examples/text_to_speech.rs][text_to_speech]]
** set-up
Unlike other examples, the TTS component currently requires various additional installations. We are working on minimizing this in the future, but for now, it is what it is. Before running an example, please go through the following steps.
- Install Miniconda. Follow [[https://docs.anaconda.com/free/miniconda/miniconda-install/][this guide]]. Once done, *on Windows* open an Anaconda Prompt terminal and proceed in this command line for all further commands. On *other OS*, proceed with a regular terminal.
- Install =rust nightly= by running the following command:
#+BEGIN_SRC sh
rustup toolchain install nightly
#+END_SRC
- *On Windows*, install C++ build tools by following [[https://github.com/bycloudai/InstallVSBuildToolsWindows][this tutorial]]. *On macOS/Ubuntu*, install LLVM instead. [[https://formulae.brew.sh/formula/llvm][Guide for macOS]]. [[https://medium.com/@donfiealex/mastering-the-process-installing-clang-llvm-on-ubuntu-22-04-bb9c69814517][Guide for Ubuntu]].
- Install =espeak-ng= by following [[https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md][this tutorial]].
- Create a new conda environment and activate it:
#+BEGIN_SRC sh
conda create -n nebula-tts-poc-venv python=3.7
conda activate nebula-tts-poc-venv
#+END_SRC
- Install =torch= using =pip=:
#+BEGIN_SRC sh
pip install torch==1.13.0 torchvision==0.14.0 torchaudio==0.13.0
#+END_SRC
** models
#+BEGIN_SRC bash
    mkdir models
    cd models
    mkdir torch-scripts
#+END_SRC
After running these commands, download models from [[https://drive.google.com/file/d/15UCvTNTwSacP_e7ZjNzib4iiHSegkM8Q/view?usp=sharing][my Google Drive]] manually. Unpack the =*.pt= files into =models/torch-scripts= folder.
** usage
#+BEGIN_SRC bash
usage:
	cargo +nightly r --release --example text_to_speech --features tts
#+END_SRC
After successful running, you should find a file =test.wav= with an audio sequence pronouncing the following =Hi! My name is Nick Chapman! Nice to meet you and all the best! I build an amazing Rust project called nebula. Would you like to participate?=.
