* [[https://github.com/nchapman/nebula/blob/main/examples/basic.rs][basic]]

** code
#+BEGIN_SRC Rust
use std::io::Write;

use nebula::{
    options::{ContextOptions, ModelOptions},
    Model,
};

fn main() {
    let args: Vec<String> = std::env::args().collect();
    if args.len() < 3 {
        println!(
            "usage:\n\t{} <model_file_path> \"<promt>\" <n_len = 150>",
            args[0]
        );
        return;
    }
    let model_file_name = args[1].clone();
    let prompt = args[2].clone();
    let mut n_len = None;
    if args.len() > 3 {
        if let Ok(ss) = str::parse(&args[3]) {
            n_len = Some(ss);
        }
    }
    let model_options = ModelOptions::default().with_n_gpu_layers(10);

    let model = Model::new(model_file_name, model_options).unwrap();

    let ctx_options = ContextOptions::default();

    let mut ctx = model.context(ctx_options).unwrap();

    ctx.eval_str(&prompt, true).unwrap();

    let mut p = ctx
        .predict()
        .with_token_callback(Box::new(|token| {
            print!("{}", token);
            std::io::stdout().flush().unwrap();
            true
        }))
        .with_temp(0.8);
    if let Some(n_len) = n_len {
        p = p.with_max_len(n_len);
    }
    p.predict().unwrap();

    println!("");
}
#+END_SRC
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example basic --features llama -- <model_file_path> "<promt>" [<n_len = 150>]
#+END_SRC
** models
*** llava-1.6-mistral-7b-gguf
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/llava-v1.6-mistral-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Mistral-7B-Instruct-v0.2-GGUF
**** Q5_K_M
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC

**** Q4_K_M
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mistral-7b-instruct-v0.2.Q5_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Mixtral-8x7B-Instruct-v0.1-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** rocket-3B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/rocket-3B-GGUF/resolve/main/rocket-3b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/rocket-3b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** zephyr-7B-beta-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/zephyr-7b-beta.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** phi-2-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/phi-2.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Yi-34B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Yi-34B-GGUF/resolve/main/yi-34b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/yi-34b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Hermes-2-Pro-Mistral-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/resolve/main/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/Hermes-2-Pro-Mistral-7B.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** gemma-7b-it
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/google/gemma-7b-it/resolve/main/gemma-7b-it.gguf
  cd -
  cargo r --release --example basic --features llama -- models/gemma-7b-it.gguf "Write helloworld code in Rust."
#+END_SRC
*** StarCoder2-15B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/second-state/StarCoder2-15B-GGUF/resolve/main/starcoder2-15b-Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/starcoder2-15b-Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** dolphin-2.6-mistral-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/dolphin-2.6-mistral-7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Nous-Hermes-2-SOLAR-10.7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF/resolve/main/nous-hermes-2-solar-10.7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/nous-hermes-2-solar-10.7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC
*** Llama-2-7B-GGUF
#+BEGIN_SRC bash
  mkdir models
  cd models
  wget https://huggingface.co/TheBloke/Llama-2-7B-GGUF/raw/resolve/llama-2-7b.Q4_K_M.gguf
  cd -
  cargo r --release --example basic --features llama -- models/llama-2-7b.Q4_K_M.gguf "Write helloworld code in Rust."
#+END_SRC


* [[https://github.com/nchapman/nebula/blob/main/examples/basic_with_image.rs][basic_with_image]]
** code
#+BEGIN_SRC
use std::io::{Read, Write};

use nebula::{
    options::{ContextOptions, ModelOptions},
    Model,
};

fn main() {
    let args: Vec<String> = std::env::args().collect();
    if args.len() < 4 {
        println!(
            "usage:\n\t{} <model_file_path> <mmproj_model_file_path> <image_file_name> \"<promt = Provide a full description.>\" <n_len = 6000>",
            args[0]
        );
        return;
    }
    let model_file_name = args[1].clone();
    let mmproj_model_file_name = args[2].clone();
    let image_file_name = args[3].clone();
    let prompt = if args.len() > 4 && !args[4].is_empty() {
        args[4].clone()
    } else {
        "Provide a full description.".to_string()
    };
    let mut n_len = 6000;
    if args.len() > 5 {
        if let Ok(ss) = str::parse(&args[5]) {
            n_len = ss;
        }
    }
    let model_options = ModelOptions::default().with_n_gpu_layers(10);
    let model =
        Model::new_with_mmproj(model_file_name, mmproj_model_file_name, model_options).unwrap();

    let context_options = ContextOptions::default().with_n_ctx(6000);
    let mut ctx = model.context(context_options).unwrap();

    //read image
    let mut image_bytes = vec![];
    let mut f = std::fs::File::open(&image_file_name).unwrap();
    f.read_to_end(&mut image_bytes).unwrap();

    //eval data
    ctx.eval_image(image_bytes, &prompt).unwrap();

    //generate predict

    //    let answer = ctx.predict(n_len).unwrap();
    //    println!("{answer}");

    //or
    ctx.predict()
        .with_token_callback(Box::new(|token| {
            print!("{}", token);
            std::io::stdout().flush().unwrap();
            true
        }))
        .with_max_len(n_len)
        .predict()
        .unwrap();
    println!("");
}
#+END_SRC
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example basic_with_image --features llama  -- <model_file_path> <mmproj_model_file_path> <image_file_name> ["<promt = <image>\nUSER:\nProvide a full description.\nASSISTANT:\n>"] [<n_len = 4000>]
#+END_SRC
** models
*** llava-1.6-mistral-7b-gguf
#+BEGIN_SRC bash
    mkdir models
    cd models
    wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/llava-v1.6-mistral-7b.Q4_K_M.gguf
    wget https://huggingface.co/cjpais/llava-1.6-mistral-7b-gguf/resolve/main/mmproj-model-f16.gguf
    cd -
    cargo r --release --example basic_with_image --features llama -- models/llava-v1.6-mistral-7b.Q4_K_M.gguf models/mmproj-model-f16.gguf ~/red-fox-300x300.jpg
#+END_SRC


* [[https://github.com/nchapman/nebula/blob/main/examples/whisper_on_wav.rs][whisper_on_wav]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example whisper_on_wav --features whisper
#+END_SRC
** models
*** ggml-base.en.bin
#+BEGIN_SRC bash
    mkdir models
    wget -O models/ggml-base.en.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin?download=true
    cargo r --release --example whisper_on_wav --features whisper
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/main/examples/whisper_stream.rs][whisper_stream]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example whisper_stream --features whisper
#+END_SRC
** models
*** ggml-base.en.bin
#+BEGIN_SRC bash
    mkdir models
    wget -O models/ggml-base.en.bin https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin?download=true
    cargo r --release --example whisper_stream --features whisper
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/PUL-122-embeddings-to-nebula/examples/embeddings.rs][embeddings]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example embeddings --features embeddings -- <model_type>
#+END_SRC
** models
*** jina
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- jina
#+END_SRC
*** t5
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- t5
#+END_SRC
*** bert
#+BEGIN_SRC bash
    cargo r --release --example embeddings --features embeddings -- bert
#+END_SRC

* [[https://github.com/nchapman/nebula/blob/main/examples/text_to_speech.rs][text_to_speech]]
** set-up
Unlike other examples, the TTS component requires a few additional installations. Before running an example, please go through the following steps.
- Install =rust nightly= by running the following command:
#+BEGIN_SRC sh
rustup toolchain install nightly
#+END_SRC
- *On Windows*, install C++ build tools by following [[https://github.com/bycloudai/InstallVSBuildToolsWindows][this tutorial]].
- *On macOS/Ubuntu*, install all additional dependencies by running the following:
#+BEGIN_SRC sh
chmod u+x build.sh
./build.sh
#+END_SRC
** models
#+BEGIN_SRC bash
    mkdir models
    cd models
    mkdir torch-scripts
    cd ..
#+END_SRC
After running these commands, download models from [[https://drive.google.com/file/d/15UCvTNTwSacP_e7ZjNzib4iiHSegkM8Q/view?usp=sharing][my Google Drive]] manually. Unpack the =*.pt= files into =models/torch-scripts= folder.
** usage
#+BEGIN_SRC bash
usage:
	cargo +nightly r --release --example text_to_speech --features tts
#+END_SRC
After successful running, you should find a file =test.wav= with an audio sequence pronouncing the following =Hi! My name is Nick Chapman! Nice to meet you and all the best! I build an amazing Rust project called nebula. Would you like to participate?=.

* [[https://github.com/nchapman/nebula/blob/PUL-383-integrate-image-generation-models/examples/text_to_image.rs][text_to_image]]
** usage
#+BEGIN_SRC bash
usage:
	cargo r --release --example text_to_image --features text-to-image -- <model_type>
#+END_SRC
** models
*** stable-diffusion
#+BEGIN_SRC bash
cargo r --release --example text_to_image --features text-to-image -- stable-diffusion
#+END_SRC
*** wuerstchen
#+BEGIN_SRC bash
cargo r --release --example text_to_image --features text-to-image -- wuerstchen
#+END_SRC
